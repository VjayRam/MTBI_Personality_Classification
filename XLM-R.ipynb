{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b0r4m0mkYuRS","outputId":"6db6aab5-8a6f-4141-9ef4-4151b2b9633f","executionInfo":{"status":"ok","timestamp":1670300935505,"user_tz":-330,"elapsed":23507,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (21.1.3)\n","Collecting pip\n","  Downloading pip-22.3.1-py3-none-any.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 6.8 MB/s \n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 21.1.3\n","    Uninstalling pip-21.1.3:\n","      Successfully uninstalled pip-21.1.3\n","Successfully installed pip-22.3.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting jupyterlab\n","  Downloading jupyterlab-3.5.1-py3-none-any.whl (8.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tomli in /usr/local/lib/python3.8/dist-packages (from jupyterlab) (2.0.1)\n","Requirement already satisfied: notebook<7 in /usr/local/lib/python3.8/dist-packages (from jupyterlab) (5.7.16)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from jupyterlab) (4.11.2)\n","Requirement already satisfied: jinja2>=2.1 in /usr/local/lib/python3.8/dist-packages (from jupyterlab) (2.11.3)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from jupyterlab) (7.9.0)\n","Collecting jupyterlab-server~=2.10\n","  Downloading jupyterlab_server-2.16.3-py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jupyter-server<3,>=1.16.0\n","  Downloading jupyter_server-1.23.3-py3-none-any.whl (346 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nbclassic\n","  Downloading nbclassic-0.4.8-py3-none-any.whl (9.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tornado>=6.1.0\n","  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.0/424.0 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from jupyterlab) (21.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2>=2.1->jupyterlab) (2.0.1)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab) (23.2.1)\n","Collecting websocket-client\n","  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting argon2-cffi\n","  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n","Requirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab) (5.7.0)\n","Collecting anyio<4,>=3.1.0\n","  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab) (0.15.0)\n","Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab) (6.1.12)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab) (0.13.3)\n","Collecting nbconvert>=6.4.4\n","  Downloading nbconvert-7.2.6-py3-none-any.whl (273 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.2/273.2 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab) (5.1.1)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.8/dist-packages (from jupyter-server<3,>=1.16.0->jupyterlab) (1.8.0)\n","Collecting json5\n","  Downloading json5-0.9.10-py2.py3-none-any.whl (19 kB)\n","Collecting jinja2>=2.1\n","  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server~=2.10->jupyterlab) (2.23.0)\n","Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server~=2.10->jupyterlab) (4.3.3)\n","Requirement already satisfied: babel in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server~=2.10->jupyterlab) (2.11.0)\n","Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server~=2.10->jupyterlab) (4.13.0)\n","Collecting notebook<7\n","  Downloading notebook-6.5.2-py3-none-any.whl (439 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.1/439.1 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: ipython-genutils in /usr/local/lib/python3.8/dist-packages (from notebook<7->jupyterlab) (0.2.0)\n","Collecting nest-asyncio>=1.5\n","  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.8/dist-packages (from notebook<7->jupyterlab) (5.3.4)\n","Collecting notebook-shim>=0.1.0\n","  Downloading notebook_shim-0.2.2-py3-none-any.whl (13 kB)\n","Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab) (2.0.10)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab) (4.8.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab) (0.2.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab) (2.6.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab) (0.7.5)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab) (4.4.2)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab) (57.4.0)\n","Collecting jedi>=0.10\n","  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->jupyterlab) (3.0.9)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.8/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.16.0->jupyterlab) (2.10)\n","Collecting sniffio>=1.1\n","  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.3->jupyterlab-server~=2.10->jupyterlab) (3.10.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->jupyterlab) (0.8.3)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab) (0.19.2)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab) (22.1.0)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab) (5.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-client>=6.1.12->jupyter-server<3,>=1.16.0->jupyterlab) (2.8.2)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab) (0.7.1)\n","Collecting tinycss2\n","  Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n","Collecting nbclient>=0.5.0\n","  Downloading nbclient-0.7.2-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab) (5.0.1)\n","Collecting jupyterlab-pygments\n","  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab) (4.6.3)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab) (1.5.0)\n","Collecting mistune<3,>=2.0.3\n","  Downloading mistune-2.0.4-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.2.0->jupyter-server<3,>=1.16.0->jupyterlab) (2.16.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->jupyterlab) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->jupyterlab) (0.2.5)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.8/dist-packages (from terminado>=0.8.3->jupyter-server<3,>=1.16.0->jupyterlab) (0.7.0)\n","Collecting argon2-cffi-bindings\n","  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.8/dist-packages (from babel->jupyterlab-server~=2.10->jupyterlab) (2022.6)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab) (3.0.4)\n","Collecting jupyter-core\n","  Downloading jupyter_core-5.1.0-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting traitlets>=5.1\n","  Downloading traitlets-5.6.0-py3-none-any.whl (107 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.5/107.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting platformdirs>=2.5\n","  Downloading platformdirs-2.5.4-py3-none-any.whl (14 kB)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=1.16.0->jupyterlab) (1.15.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server<3,>=1.16.0->jupyterlab) (0.5.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=1.16.0->jupyterlab) (2.21)\n","Installing collected packages: mistune, json5, websocket-client, traitlets, tornado, tinycss2, sniffio, platformdirs, nest-asyncio, jupyterlab-pygments, jinja2, jedi, jupyter-core, argon2-cffi-bindings, anyio, argon2-cffi, nbclient, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, nbclassic, notebook, jupyterlab\n","  Attempting uninstall: mistune\n","    Found existing installation: mistune 0.8.4\n","    Uninstalling mistune-0.8.4:\n","      Successfully uninstalled mistune-0.8.4\n","  Attempting uninstall: traitlets\n","    Found existing installation: traitlets 5.1.1\n","    Uninstalling traitlets-5.1.1:\n","      Successfully uninstalled traitlets-5.1.1\n","  Attempting uninstall: tornado\n","    Found existing installation: tornado 6.0.4\n","    Uninstalling tornado-6.0.4:\n","      Successfully uninstalled tornado-6.0.4\n","  Attempting uninstall: jinja2\n","    Found existing installation: Jinja2 2.11.3\n","    Uninstalling Jinja2-2.11.3:\n","      Successfully uninstalled Jinja2-2.11.3\n","  Attempting uninstall: jupyter-core\n","    Found existing installation: jupyter_core 4.11.2\n","    Uninstalling jupyter_core-4.11.2:\n","      Successfully uninstalled jupyter_core-4.11.2\n","  Attempting uninstall: nbconvert\n","    Found existing installation: nbconvert 5.6.1\n","    Uninstalling nbconvert-5.6.1:\n","      Successfully uninstalled nbconvert-5.6.1\n","  Attempting uninstall: notebook\n","    Found existing installation: notebook 5.7.16\n","    Uninstalling notebook-5.7.16:\n","      Successfully uninstalled notebook-5.7.16\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires notebook~=5.7.16, but you have notebook 6.5.2 which is incompatible.\n","google-colab 1.0.0 requires tornado~=6.0.4, but you have tornado 6.2 which is incompatible.\n","flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed anyio-3.6.2 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 jedi-0.18.2 jinja2-3.1.2 json5-0.9.10 jupyter-core-5.1.0 jupyter-server-1.23.3 jupyterlab-3.5.1 jupyterlab-pygments-0.2.2 jupyterlab-server-2.16.3 mistune-2.0.4 nbclassic-0.4.8 nbclient-0.7.2 nbconvert-7.2.6 nest-asyncio-1.5.6 notebook-6.5.2 notebook-shim-0.2.2 platformdirs-2.5.4 sniffio-1.3.0 tinycss2-1.2.1 tornado-6.2 traitlets-5.6.0 websocket-client-1.4.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install --upgrade pip\n","!pip3 install jupyterlab"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dH-MqGVXY4AZ","outputId":"deb6ccb1-e6a6-443b-ec23-7c9d45f5fd99","executionInfo":{"status":"ok","timestamp":1670301008075,"user_tz":-330,"elapsed":72579,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.10.2\n","  Downloading torch-1.10.2-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1102422016 bytes == 0x36ef8000 @  0x7fc82895a615 0x5d631c 0x51e4f1 0x51e67b 0x4f7585 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1 0x49ca7c 0x5d7c18 0x49ca7c 0x55e571 0x5d7cf1 0x49ced5 0x55e571 0x5d7cf1 0x49ca7c 0x55e571 0x5d7cf1\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.2) (4.1.1)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.10.2 which is incompatible.\n","torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.10.2 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.10.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-1.10.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install \"torch == 1.10.2\""]},{"cell_type":"code","execution_count":3,"metadata":{"id":"0Khp_CIKZIam","executionInfo":{"status":"ok","timestamp":1670301009572,"user_tz":-330,"elapsed":1518,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[],"source":["import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y7K75MNDZd1V","outputId":"2478b824-3115-46cd-9695-cf18ca0255b0","executionInfo":{"status":"ok","timestamp":1670301010065,"user_tz":-330,"elapsed":5,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","GPU is: Tesla T4\n"]}],"source":["import torch\n","# check if we have cuda installed\n","if torch.cuda.is_available():\n","    # to use GPU\n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('GPU is:', torch.cuda.get_device_name(0))\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":348},"id":"VhoyGGQWZh0X","outputId":"ad47067a-17fa-4c68-a63c-16ddc136c412","executionInfo":{"status":"ok","timestamp":1670301015107,"user_tz":-330,"elapsed":5045,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]},{"output_type":"execute_result","data":{"text/plain":["   type                                              posts\n","0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n","1  ENTP  'I'm finding the lack of me in these posts ver...\n","2  INTP  'Good one  _____   https://www.youtube.com/wat...\n","3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n","4  ENTJ  'You're fired.|||That's another silly misconce..."],"text/html":["\n","  <div id=\"df-002b64ed-349c-49f0-a33a-9f4ec7a9a035\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>posts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>INFJ</td>\n","      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ENTP</td>\n","      <td>'I'm finding the lack of me in these posts ver...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>INTP</td>\n","      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>INTJ</td>\n","      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ENTJ</td>\n","      <td>'You're fired.|||That's another silly misconce...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-002b64ed-349c-49f0-a33a-9f4ec7a9a035')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-002b64ed-349c-49f0-a33a-9f4ec7a9a035 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-002b64ed-349c-49f0-a33a-9f4ec7a9a035');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}],"source":["!pip install pandas \n","# read in train dataset\n","import pandas as pd\n","df = pd.read_csv('/content/mbti_1.csv')\n","df.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pg6kFs4LbqEv","executionInfo":{"status":"ok","timestamp":1670301026016,"user_tz":-330,"elapsed":10913,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}},"outputId":"b6e2bf78-c19d-48e1-99c2-9d10ff62d119"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install re\n","import re\n","import string\n","def clean_text(text):\n","    # to lower case\n","    text = text.lower()\n","    # remove links\n","    text = re.sub('https:\\/\\/\\S+', '', text) \n","    # remove punctuation\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) \n","    # remove next line     \n","    text = re.sub(r'[^ \\w\\.]', '', text) \n","    # remove words containing numbers\n","    text = re.sub('\\w*\\d\\w*', '', text)\n","    \n","    return text\n","# Create a new column called \"Text\" for collecting clean text\n","df['text'] = df.posts.apply(lambda x: clean_text(x))"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"IaHGsNTBdax9","executionInfo":{"status":"ok","timestamp":1670301026017,"user_tz":-330,"elapsed":16,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}},"outputId":"9b8285c4-2259-4157-ad4b-b57995158ba1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   type                                              posts  \\\n","0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...   \n","1  ENTP  'I'm finding the lack of me in these posts ver...   \n","2  INTP  'Good one  _____   https://www.youtube.com/wat...   \n","3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n","4  ENTJ  'You're fired.|||That's another silly misconce...   \n","\n","                                                text  \n","0   and intj moments    sportscenter not top ten ...  \n","1  im finding the lack of me in these posts very ...  \n","2  good one      course to which i say i know tha...  \n","3  dear intp   i enjoyed our conversation the oth...  \n","4  youre firedthats another silly misconception t...  "],"text/html":["\n","  <div id=\"df-9ee5defb-8246-4874-84c8-baa8139ab5b3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>posts</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>INFJ</td>\n","      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n","      <td>and intj moments    sportscenter not top ten ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ENTP</td>\n","      <td>'I'm finding the lack of me in these posts ver...</td>\n","      <td>im finding the lack of me in these posts very ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>INTP</td>\n","      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n","      <td>good one      course to which i say i know tha...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>INTJ</td>\n","      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n","      <td>dear intp   i enjoyed our conversation the oth...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ENTJ</td>\n","      <td>'You're fired.|||That's another silly misconce...</td>\n","      <td>youre firedthats another silly misconception t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ee5defb-8246-4874-84c8-baa8139ab5b3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9ee5defb-8246-4874-84c8-baa8139ab5b3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9ee5defb-8246-4874-84c8-baa8139ab5b3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}],"source":["df.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vfu_gRdFdfnN","executionInfo":{"status":"ok","timestamp":1670301038064,"user_tz":-330,"elapsed":12061,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}},"outputId":"2752d301-70cf-4302-d803-9af276e768e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==4.16.2\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.16.2) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.16.2) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.16.2) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.16.2) (4.64.1)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.16.2) (2022.6.2)\n","Collecting tokenizers!=0.11.3,>=0.10.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.16.2) (3.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.16.2) (2.23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers==4.16.2) (3.0.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.16.2) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.16.2) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.16.2) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.16.2) (2022.9.24)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.16.2) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.16.2) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.16.2) (1.2.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=fcb0b6cc32440cfaedde6f450dad385a954db17ef52e2edc34d8b081f8540161\n","  Stored in directory: /root/.cache/pip/wheels/64/a3/ff/01dc060d7fc51176b3ce7cf1561466a12e658164b594747547\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 sacremoses-0.0.53 tokenizers-0.13.2 transformers-4.16.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install \"transformers ==4.16.2\" "]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kyFNSzeXdtFj","executionInfo":{"status":"ok","timestamp":1670301042723,"user_tz":-330,"elapsed":4671,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}},"outputId":"af6c9c39-f5ba-4308-c5fe-94d6a3c1559c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece==0.1.96\n","  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install \"sentencepiece==0.1.96\""]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["a88379c557024c6fbe9fe6e8401bc2a6","741c65261acd45dcacc27a9630f094c6","454747f688c34db7abdea0810e6601fe","0cdcdc1123f24ad6841f1b51226413bc","d81af0fc640347619ef0601ea0439227","e634e78715564f9ebdf910eaaf5a6a9f","f18818cb5dac41d1b7d148aab3d6913b","ccf223a5dc3e471e96d66a7127e03142","1eff10043b5a41d2b6583b8600220d6c","cbc600720a3449918604396dc918ec6e","eda3feb45ecb48d194623b4c4cdd8bc9","5ff29e3820b745488b20685e5033dbff","dbd8c885216840d4841f98ca01021590","ac15a09958c14ed58f82c3003a6d3fa0","e651f5860cf14781860a4180da368d16","62380449291b4602993dc2f894447c58","4e009b1bcd074ea8952fd24f32f1bf97","758ca90e47244817b4ea7d03105a302b","cc1b7afb61244b458cab1f9845199aa5","1e62c8a9cc1f44aabef4631c36997a19","02279955297d43ca8d0a2788cf537076","45c7dcba7cfa4e5b86aba5ca0988c37e","fda59624eaac4ea2b9a436cdf2b3908a","78dcc8c8a8d842d5af90bd674dd0f185","55a787a6db504a028808d4a6c98bb804","aacfe5edc9e94708893343fe6e5baeaf","026fd1675c674c66833174a847cef10b","03010caffffe45cc84018537f5f0a95f","3a34f74a618344d28ab345566934efea","c47f51a8c56141d28a50495d73ee622e","664b6903415f4d4181342e0d5f68afd0","a086ff11bb5b4fe3ab9b0920edf99bf7","8b87e7637dc940faa6bfd48dec4e435d"]},"id":"BtGR7U8id1Cz","executionInfo":{"status":"ok","timestamp":1670301048952,"user_tz":-330,"elapsed":6232,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}},"outputId":"b0c07452-4b14-4534-8a3e-23f783b152c5"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a88379c557024c6fbe9fe6e8401bc2a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff29e3820b745488b20685e5033dbff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/8.68M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fda59624eaac4ea2b9a436cdf2b3908a"}},"metadata":{}}],"source":["from transformers import AutoTokenizer\n","tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gO-GeBeId-co","executionInfo":{"status":"ok","timestamp":1670301048953,"user_tz":-330,"elapsed":7,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}},"outputId":"566f437a-26cd-497e-9021-65ee03bc29ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'and': [136], 'intj': [23, 18, 170], 'moments': [53189], 'sportscenter': [29681, 30090], 'not': [959], 'top': [2663], 'ten': [1492], 'plays': [11301, 7], 'prankswhat': [1645, 19, 1224, 124409], 'has': [1556], 'been': [2809], 'the': [70], 'most': [2684], 'lifechanging': [6897, 90965, 214], 'experience': [16981], 'in': [23], 'your': [935], 'on': [98], 'repeat': [119140], 'for': [100], 'of': [111], 'todaymay': [18925, 11354], 'perc': [21778], 'immerse': [3807, 184], 'youthe': [398, 2347], 'last': [4568], 'thing': [13580], 'my': [759], 'infj': [34128, 170], 'friend': [34391], 'posted': [44183], 'his': [1919], 'facebook': [10899], 'before': [8108], 'committing': [375, 266, 19514], 'suicide': [193341], 'next': [11737], 'day': [5155], 'rest': [10588], 'peace': [88669], 'sorry': [59090], 'to': [47], 'hear': [36802], 'distress': [45, 95650], 'its': [6863], 'only': [4734], 'natural': [6083], 'a': [10], 'relationship': [76755], 'be': [186], 'perfection': [157799], 'all': [756], 'time': [1733], 'every': [11907], 'moment': [3095], 'existence': [6, 116311], 'try': [9790], 'figure': [26366], 'hard': [7941], 'times': [20028], 'as': [237], 'growth': [75678], 'welcome': [81907], 'game': [6712], 'set': [5423], 'matchprozac': [14858, 3454, 596, 238], 'wellbrutin': [5299, 11911, 2311], 'at': [99], 'least': [19713], 'thirty': [6, 95037, 939], 'minutes': [14633], 'moving': [98567], 'legs': [6049, 7], 'i': [17], 'dont': [13319], 'mean': [29459], 'them': [2856], 'while': [12960], 'sitting': [129842], 'same': [5701], 'desk': [8, 1042], 'chair': [80923], 'weed': [642, 297], 'moderation': [32610, 2320], 'maybe': [51139], 'edibles': [6726, 1577], 'healthier': [16227, 6815], 'alternativebasically': [30700, 5918, 71407], 'come': [1380], 'up': [1257], 'with': [678], 'three': [17262], 'items': [55769], 'youve': [398, 272], 'determined': [83324, 71], 'that': [450], 'each': [12638], 'type': [10644], 'or': [707], 'whichever': [3129, 30441], 'types': [52895], 'you': [398], 'want': [3444], 'do': [54], 'would': [2806], 'more': [1286], 'than': [3501], 'likely': [47041], 'use': [4527], 'given': [34475], 'cognitive': [241761], 'functions': [32354, 7], 'whatnot': [2367, 10869], 'when': [3229], 'left': [25737], 'byall': [390, 5584], 'things': [8966], 'sims': [10777, 7], 'is': [83], 'indeed': [160463], 'video': [1202], 'good': [4127], 'one': [1632], 'note': [20537], 'somewhat': [208806], 'subjective': [28368, 5844], 'am': [444], 'completely': [64557], 'promoting': [8891, 1916], 'death': [47219], 'any': [2499], 'simdear': [10777, 112, 147], 'enfp': [22, 420, 254], 'what': [2367], 'were': [3542], 'favorite': [40304], 'games': [27528], 'growing': [105925], 'are': [621], 'now': [5036], 'current': [43581], 'cool': [21185], 'appears': [135179], 'too': [5792], 'late': [72399], 'sadtheres': [17110, 9319, 90], 'someone': [22008], 'out': [1810], 'there': [2685], 'everyonewait': [30309, 634, 217], 'thought': [17569], 'confidence': [159454], 'was': [509], 'thingi': [13580, 14], 'just': [1660], 'cherish': [290, 16507], 'solitude': [3115, 21753, 13], 'bc': [6, 65037], 'revel': [456, 2601], 'within': [28032], 'inner': [75414], 'world': [8999], 'whereas': [7440, 162], 'other': [3789], 'id': [3447], 'workin': [4488, 73], 'enjoy': [25225], 'me': [163], 'can': [831], 'worry': [90908], 'people': [3395], 'will': [1221], 'always': [11343], 'around': [10932], 'toyo': [47, 1410], 'entp': [22, 18, 254], 'ladies': [21, 34204], 'if': [2174], 'youre': [935, 13], 'into': [3934], 'complimentary': [75870, 6635], 'personalitywell': [3357, 2481, 19256], 'hey': [28192], 'main': [5201], 'social': [2265], 'outlet': [148053], 'xbox': [1022, 11728], 'live': [6867], 'conversations': [77104, 7], 'even': [3853], 'then': [7068], 'verbally': [78975, 538], 'fatigue': [209358], 'really': [6183], 'dig': [1466], 'part': [2831], 'from': [1295], 'because': [6637], 'this': [903], 'thread': [86997], 'requires': [144570], 'it': [442], 'meget': [4610], 'high': [11192], 'backyard': [4420, 119300], 'roast': [2062, 4438], 'eat': [73203], 'marshmellows': [1108, 7088, 41566, 7], 'conversing': [158, 7864, 214], 'over': [645], 'something': [9844], 'intellectual': [91768, 289], 'followed': [134629], 'by': [390], 'massages': [5945, 7], 'many': [5941], 'bs': [876, 7], 'sentence': [149357], 'how': [3642], 'could': [5809], 'think': [5351], 'bbanned': [876, 761, 14534], 'watching': [100244], 'movies': [72304], 'corner': [107767], 'duncesbanned': [7524, 5170, 761, 14534], 'health': [16227], 'class': [18507], 'clearly': [123019], 'taught': [189924], 'nothing': [33720], 'about': [1672], 'peer': [280, 56], 'pressurebanned': [81147, 761, 14534], 'whole': [28271], 'host': [27980], 'two': [6626], 'baby': [15546], 'deer': [8, 56], 'right': [7108], 'munching': [4829, 59207], 'beetle': [186, 126, 133], 'middle': [86991], 'using': [17368], 'their': [2363], 'own': [10002], 'blood': [59714], 'cavemen': [143383, 1055], 'diary': [45, 6635], 'todays': [18925, 7], 'latest': [42850], 'happenings': [123087, 7], 'designated': [4331, 27686], 'cave': [143383], 'wall': [58982], 'see': [1957], 'asa': [11096], 'pokemon': [21250, 74768], 'an': [142], 'society': [100510], 'everyone': [30309], 'becomes': [24209, 7], 'artists': [22104, 7], 'they': [1836], 'draw': [79442], 'idea': [6528], 'counts': [54529, 7], 'forming': [3173, 214], 'like': [1884], 'signaturewelcome': [138256, 8420, 45738], 'robot': [11329], 'ranks': [30648, 7], 'person': [3445], 'who': [2750], 'downed': [7565, 297], 'selfesteem': [15970, 4896, 195], 'cuz': [314, 169], 'im': [566], 'avid': [10, 5518], 'signature': [138256], 'artist': [22104], 'herself': [164055], 'proudbanned': [79961, 761, 14534], 'taking': [35971], 'room': [17155], 'under': [1379], 'bed': [11958], 'ya': [151], 'gotta': [17763, 11], 'learn': [30698], 'share': [12008], 'being': [8035], 'much': [5045], 'thundering': [4911, 7944, 214], 'grumbling': [23322, 39, 79298], 'kind': [8562], 'storm': [77076], 'yepahh': [2422, 61118, 127], 'old': [10332], 'school': [10696], 'music': [19612], 'havent': [765, 660], 'heard': [49782], 'ages': [10, 4188], 'failed': [165523], 'public': [3835], 'speaking': [142146], 'few': [10846], 'years': [5369], 'ago': [6650], 'ive': [17, 272], 'sort': [12096], 'learned': [97384], 'better': [11522], 'position': [19069], 'again': [13438], 'big': [6957], 'failure': [137578], 'overloading': [645, 63033, 214], 'myself': [35978], 'tooi': [5792, 14], 'persons': [3445, 7], 'mentality': [13893, 2481], 'hes': [764, 7], 'confirmed': [39563, 297], 'way': [3917], 'denver': [168, 814], 'area': [16128], 'start': [4034], 'new': [3525], 'life': [6897]}\n"]}],"source":["print({x : tokenizer.encode(x, add_special_tokens=False) for x in df.text.values[0].split()})"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":739},"id":"urnN5rZIjBw-","executionInfo":{"status":"ok","timestamp":1670301092785,"user_tz":-330,"elapsed":43836,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}},"outputId":"c072f46e-015d-4b13-e89b-fc3e9e9e8ecb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.21.6)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]},{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["max:  2184\n","min:  3\n"]},{"output_type":"execute_result","data":{"text/plain":["(array([   0.,  500., 1000., 1500., 2000., 2500.]),\n"," <a list of 6 Text major ticklabel objects>)"]},"metadata":{},"execution_count":12},{"output_type":"display_data","data":{"text/plain":["<Figure size 1440x576 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# install matplotlib\n","!pip install matplotlib\n","# tokenize the text feature \n","tokenized_feature_raw = tokenizer.batch_encode_plus(\n","                            # Sentences to encode\n","                            df.text.values.tolist(), \n","                            # Add '[CLS]' and '[SEP]'\n","                            add_special_tokens = True      \n","                   )\n","# collect tokenized sentence length \n","token_sentence_length = [len(x) for x in tokenized_feature_raw['input_ids']]\n","print('max: ', max(token_sentence_length))\n","print('min: ', min(token_sentence_length))\n","# plot the distribution\n","import matplotlib.pyplot as plt\n","plt.figure(figsize=(20, 8))\n","plt.hist(token_sentence_length, rwidth = 0.9)\n","plt.xlabel('Sequence Length', fontsize = 18)\n","plt.ylabel('# of Samples', fontsize = 18)\n","plt.xticks(fontsize = 14)\n","plt.yticks(fontsize = 14)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"w-eaF7qym_N-","executionInfo":{"status":"ok","timestamp":1670301092786,"user_tz":-330,"elapsed":16,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[],"source":["# identify features and target\n","features = df.text.values.tolist()\n","target = df.type.values.tolist()"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"USjxl6yqnbrf","executionInfo":{"status":"ok","timestamp":1670301128950,"user_tz":-330,"elapsed":36179,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[],"source":["# tokenize features \n","MAX_LEN = 128\n","tokenized_feature = tokenizer.batch_encode_plus(\n","                            # Sentences to encode\n","                            features, \n","                            # Add '[CLS]' and '[SEP]'\n","                            add_special_tokens = True,\n","                            # Add empty tokens if len(text)<MAX_LEN\n","                            padding = 'max_length',\n","                            # Truncate all sentences to max length\n","                            truncation=True,\n","                            # Set the maximum length\n","                            max_length = MAX_LEN, \n","                            # Return attention mask\n","                            return_attention_mask = True,\n","                            # Return pytorch tensors\n","                            return_tensors = 'pt'       \n","                   )"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yL7X8tXFnkBM","executionInfo":{"status":"ok","timestamp":1670301134109,"user_tz":-330,"elapsed":5172,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}},"outputId":"1a89ac38-7ed9-43c5-9806-a892a54e7c22"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sklearn\n","  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: sklearn\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=234b3e8ca9d86aba5387439a484591025d1a2396078ec362744a54aedbac99e6\n","  Stored in directory: /root/.cache/pip/wheels/1c/2f/26/476423e3abcbdc095c9061b4a385339f4d5c4952c036ef8262\n","Successfully built sklearn\n","Installing collected packages: sklearn\n","Successfully installed sklearn-0.0.post1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["# install sklearn \n","!pip install sklearn\n","# convert label into numeric \n","from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","le.fit(target)\n","target_num = le.transform(target)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qL7V6MLznpDa","executionInfo":{"status":"ok","timestamp":1670301134110,"user_tz":-330,"elapsed":15,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}},"outputId":"36f7b57a-22ed-4006-bf08-0362e1cd2484"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8674,)"]},"metadata":{},"execution_count":16}],"source":["target_num.shape"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Uq8UW6zznw6c","executionInfo":{"status":"ok","timestamp":1670301134110,"user_tz":-330,"elapsed":13,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[],"source":["# Use 80% for training and 20% for validation\n","from sklearn.model_selection import train_test_split\n","train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(tokenized_feature['input_ids'], \n","                                                                                                             target_num,\n","                                                                                                                    tokenized_feature['attention_mask'],\n","                                                                                                      random_state=2018, test_size=0.2, stratify=target)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"CMsKDodwoJHu","executionInfo":{"status":"ok","timestamp":1670301134110,"user_tz":-330,"elapsed":13,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[],"source":["# define batch_size\n","batch_size = 16\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, torch.tensor(train_labels))\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","# Create the DataLoader for our test set\n","validation_data = TensorDataset(validation_inputs, validation_masks, torch.tensor(validation_labels))\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["04d9a60b72744841958906264a713c4b","3dd95a7424c544dc895a08c8bf263359","d6d7a35a8c394396b400221d092d18ec","ee0772b9800d43848528ed163055b076","c79d743e61b64098ad21929c7885eedb","5f203d0925c346738d229e815543c882","c10716b0f18d45779828953676bf2efc","57f3f24bd7554cdd8be6e04472ac1335","ace0a125c08240dcbdc044bdfbdbb7e9","e9ce49b808de43e5bbe86cd75871a192","358be511747d421d9b8ab024aac0917f"]},"id":"DzlhMzVToNJs","executionInfo":{"status":"ok","timestamp":1670301198066,"user_tz":-330,"elapsed":63969,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}},"outputId":"48a90c12-8430-414f-c8ee-95af8e45b969"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04d9a60b72744841958906264a713c4b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# BertForSequenceClassification\n","from transformers import XLMRobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","model = XLMRobertaForSequenceClassification.from_pretrained(\n","    \"xlm-roberta-base\", \n","    # Specify number of classes\n","    num_labels = len(set(target)), \n","    # Whether the model returns attentions weights\n","    output_attentions = False,\n","    # Whether the model returns all hidden-states \n","    output_hidden_states = False\n",")"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdpOSynYoPmE","executionInfo":{"status":"ok","timestamp":1670301198066,"user_tz":-330,"elapsed":18,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}},"outputId":"e3ab687c-0ae6-45df-fb96-9c1993b0e5ca"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(250002, 768, padding_idx=1)"]},"metadata":{},"execution_count":20}],"source":["# Receive the full size of the new word\n","model.resize_token_embeddings(len(tokenizer))"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1w2PLVXgodrT","executionInfo":{"status":"ok","timestamp":1670301198067,"user_tz":-330,"elapsed":16,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}},"outputId":"6b078f71-a32a-4a1e-841b-a852813d4397"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["# Optimizer & Learning Rate Scheduler\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, \n","                  eps = 1e-8 \n","                )"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"u8YhBSVSogwA","executionInfo":{"status":"ok","timestamp":1670301198067,"user_tz":-330,"elapsed":14,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[],"source":["# Number of training epochs\n","epochs = 4\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","# Create the learning rate scheduler\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U2FLAeNjoll1","executionInfo":{"status":"ok","timestamp":1670301207878,"user_tz":-330,"elapsed":9825,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}},"outputId":"8a4b6f09-6d87-4992-f367-40ac208c023c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["XLMRobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=16, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":23}],"source":["model.cuda()"]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HoQdsSJ0sQQl","executionInfo":{"status":"ok","timestamp":1670301468767,"user_tz":-330,"elapsed":389,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}},"outputId":"de3e7a30-3e66-49bf-e6cf-55e3ae2a0854"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["XLMRobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=16, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WqSM6Lhwpgt-","executionInfo":{"status":"aborted","timestamp":1670301208554,"user_tz":-330,"elapsed":9,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[],"source":["enumerate(train_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Se3foBqroo_R","executionInfo":{"status":"aborted","timestamp":1670301208554,"user_tz":-330,"elapsed":9,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[],"source":["# Training\n","import time\n","# Store the average loss after each epoch \n","loss_values = []\n","# number of total steps for each epoch\n","print('total steps per epoch: ',  len(train_dataloader) / batch_size)\n","# looping over epochs\n","for epoch_i in range(0, epochs):\n","    \n","    print('training on epoch: ', epoch_i)\n","    # set start time \n","    t0 = time.time()\n","    # reset total loss\n","    total_loss = 0\n","    # model in training \n","    model.train()\n","    # loop through batch \n","    for step, batch in enumerate(train_dataloader):\n","        # Progress update every 50 step \n","        if step % 50 == 0 and not step == 0:\n","            print('training on step: ', step)\n","            print('total time used is: {0:.2f} s'.format(time.time() - t0))\n","        # load data from dataloader \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        print(b_input_ids.shape)\n","        # clear any previously calculated gradients \n","        model.zero_grad()\n","        # get outputs\n","        outputs = model(b_input_ids,\n","                        token_type_ids=None,\n","                        attention_mask=b_input_mask,\n","                        labels=b_labels)\n","        # get loss\n","        loss = outputs[0]\n","        # total loss\n","        total_loss += loss.item()\n","        # clip the norm of the gradients to 1.0.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        # update optimizer\n","        optimizer.step()\n","        # update learning rate \n","        scheduler.step()\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)\n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","    print(\"average training loss: {0:.2f}\".format(avg_train_loss))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kkQYpvSCozOd","executionInfo":{"status":"aborted","timestamp":1670301208554,"user_tz":-330,"elapsed":9,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[],"source":["# Test\n","import numpy as np\n","t0 = time.time()\n","# model in validation mode\n","model.eval()\n","# save prediction\n","predictions,true_labels =[],[]\n","# evaluate data for one epoch\n","for batch in validation_dataloader:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # validation\n","    with torch.no_grad():\n","        outputs = model(b_input_ids,\n","                        token_type_ids=None,\n","                        attention_mask=b_input_mask)\n","    # get output\n","    logits = outputs[0]\n","    # move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","    final_prediction = np.argmax(logits, axis=-1).flatten()\n","    predictions.append(final_prediction)\n","    true_labels.append(label_ids)\n","    \n","print('total time used is: {0:.2f} s'.format(time.time() - t0))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KC0LtKo4tCI1","executionInfo":{"status":"aborted","timestamp":1670301208555,"user_tz":-330,"elapsed":10,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[],"source":["predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMbmWl0xtDFA","executionInfo":{"status":"aborted","timestamp":1670301208555,"user_tz":-330,"elapsed":10,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[],"source":["# convert numeric label to string\n","final_prediction_list = le.inverse_transform(np.concatenate(predictions))\n","final_truelabel_list = le.inverse_transform(np.concatenate(true_labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crKn4jhMtMH1","executionInfo":{"status":"aborted","timestamp":1670301208556,"user_tz":-330,"elapsed":11,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[],"source":["final_prediction_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9g_5UhEtO95","executionInfo":{"status":"aborted","timestamp":1670301208556,"user_tz":-330,"elapsed":11,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, classification_report\n","cr = classification_report(final_truelabel_list, \n","                           final_prediction_list, \n","                           output_dict=False)\n","print(cr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IF40FkkitWWO","executionInfo":{"status":"aborted","timestamp":1670301208557,"user_tz":-330,"elapsed":10,"user":{"displayName":"Vijay Ram Enaganti","userId":"10097920489639571138"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyOPR9SObPsxlrb1uHql876d"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a88379c557024c6fbe9fe6e8401bc2a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_741c65261acd45dcacc27a9630f094c6","IPY_MODEL_454747f688c34db7abdea0810e6601fe","IPY_MODEL_0cdcdc1123f24ad6841f1b51226413bc"],"layout":"IPY_MODEL_d81af0fc640347619ef0601ea0439227"}},"741c65261acd45dcacc27a9630f094c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e634e78715564f9ebdf910eaaf5a6a9f","placeholder":"​","style":"IPY_MODEL_f18818cb5dac41d1b7d148aab3d6913b","value":"Downloading: 100%"}},"454747f688c34db7abdea0810e6601fe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ccf223a5dc3e471e96d66a7127e03142","max":615,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1eff10043b5a41d2b6583b8600220d6c","value":615}},"0cdcdc1123f24ad6841f1b51226413bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbc600720a3449918604396dc918ec6e","placeholder":"​","style":"IPY_MODEL_eda3feb45ecb48d194623b4c4cdd8bc9","value":" 615/615 [00:00&lt;00:00, 19.4kB/s]"}},"d81af0fc640347619ef0601ea0439227":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e634e78715564f9ebdf910eaaf5a6a9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f18818cb5dac41d1b7d148aab3d6913b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ccf223a5dc3e471e96d66a7127e03142":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1eff10043b5a41d2b6583b8600220d6c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cbc600720a3449918604396dc918ec6e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eda3feb45ecb48d194623b4c4cdd8bc9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ff29e3820b745488b20685e5033dbff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dbd8c885216840d4841f98ca01021590","IPY_MODEL_ac15a09958c14ed58f82c3003a6d3fa0","IPY_MODEL_e651f5860cf14781860a4180da368d16"],"layout":"IPY_MODEL_62380449291b4602993dc2f894447c58"}},"dbd8c885216840d4841f98ca01021590":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e009b1bcd074ea8952fd24f32f1bf97","placeholder":"​","style":"IPY_MODEL_758ca90e47244817b4ea7d03105a302b","value":"Downloading: 100%"}},"ac15a09958c14ed58f82c3003a6d3fa0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc1b7afb61244b458cab1f9845199aa5","max":5069051,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e62c8a9cc1f44aabef4631c36997a19","value":5069051}},"e651f5860cf14781860a4180da368d16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02279955297d43ca8d0a2788cf537076","placeholder":"​","style":"IPY_MODEL_45c7dcba7cfa4e5b86aba5ca0988c37e","value":" 4.83M/4.83M [00:00&lt;00:00, 10.7MB/s]"}},"62380449291b4602993dc2f894447c58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e009b1bcd074ea8952fd24f32f1bf97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"758ca90e47244817b4ea7d03105a302b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc1b7afb61244b458cab1f9845199aa5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e62c8a9cc1f44aabef4631c36997a19":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"02279955297d43ca8d0a2788cf537076":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45c7dcba7cfa4e5b86aba5ca0988c37e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fda59624eaac4ea2b9a436cdf2b3908a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_78dcc8c8a8d842d5af90bd674dd0f185","IPY_MODEL_55a787a6db504a028808d4a6c98bb804","IPY_MODEL_aacfe5edc9e94708893343fe6e5baeaf"],"layout":"IPY_MODEL_026fd1675c674c66833174a847cef10b"}},"78dcc8c8a8d842d5af90bd674dd0f185":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03010caffffe45cc84018537f5f0a95f","placeholder":"​","style":"IPY_MODEL_3a34f74a618344d28ab345566934efea","value":"Downloading: 100%"}},"55a787a6db504a028808d4a6c98bb804":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c47f51a8c56141d28a50495d73ee622e","max":9096718,"min":0,"orientation":"horizontal","style":"IPY_MODEL_664b6903415f4d4181342e0d5f68afd0","value":9096718}},"aacfe5edc9e94708893343fe6e5baeaf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a086ff11bb5b4fe3ab9b0920edf99bf7","placeholder":"​","style":"IPY_MODEL_8b87e7637dc940faa6bfd48dec4e435d","value":" 8.68M/8.68M [00:00&lt;00:00, 17.5MB/s]"}},"026fd1675c674c66833174a847cef10b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03010caffffe45cc84018537f5f0a95f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a34f74a618344d28ab345566934efea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c47f51a8c56141d28a50495d73ee622e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"664b6903415f4d4181342e0d5f68afd0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a086ff11bb5b4fe3ab9b0920edf99bf7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b87e7637dc940faa6bfd48dec4e435d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"04d9a60b72744841958906264a713c4b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3dd95a7424c544dc895a08c8bf263359","IPY_MODEL_d6d7a35a8c394396b400221d092d18ec","IPY_MODEL_ee0772b9800d43848528ed163055b076"],"layout":"IPY_MODEL_c79d743e61b64098ad21929c7885eedb"}},"3dd95a7424c544dc895a08c8bf263359":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f203d0925c346738d229e815543c882","placeholder":"​","style":"IPY_MODEL_c10716b0f18d45779828953676bf2efc","value":"Downloading: 100%"}},"d6d7a35a8c394396b400221d092d18ec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_57f3f24bd7554cdd8be6e04472ac1335","max":1115590446,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ace0a125c08240dcbdc044bdfbdbb7e9","value":1115590446}},"ee0772b9800d43848528ed163055b076":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9ce49b808de43e5bbe86cd75871a192","placeholder":"​","style":"IPY_MODEL_358be511747d421d9b8ab024aac0917f","value":" 1.04G/1.04G [00:51&lt;00:00, 27.1MB/s]"}},"c79d743e61b64098ad21929c7885eedb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f203d0925c346738d229e815543c882":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c10716b0f18d45779828953676bf2efc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57f3f24bd7554cdd8be6e04472ac1335":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ace0a125c08240dcbdc044bdfbdbb7e9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e9ce49b808de43e5bbe86cd75871a192":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"358be511747d421d9b8ab024aac0917f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}